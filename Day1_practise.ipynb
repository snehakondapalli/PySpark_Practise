{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88297306-61f7-4792-b45c-0d012f27d687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise 1: Spark Setup and RDD Basics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56debb01-9821-43a4-b643-e4cd94939e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Created Successfully!\nSpark Version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark Core Concepts\").master(\"local[*]\").getOrCreate()  \n",
    "print(\"Spark Session Created Successfully!\")\n",
    "print(\"Spark Version:\", spark.version)  # Check Spark version\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b9ba873-52df-4f10-a596-d9aa79e15d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "** Create an RDD: Parallelize a small Python collection (e.g., a list of 8–10 numbers or strings) into an RDD using spark.sparkContext.parallelize(). Print out the number of partitions of the RDD (use RDD.getNumPartitions()) to see how the data is split.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "669c2e1b-1ed2-4abb-a181-6ffbd92f33c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions for numerical data is: 8\nNumber of partitions for text data is: 8\n"
     ]
    }
   ],
   "source": [
    "data = [10,20,30,40,50,60,70,80,90,100]\n",
    "text = ['python', 'scala', 'sql', 'databricks', 'spark', 'aws', 'azure', 'machine learning']\n",
    "rdd_data = spark.sparkContext.parallelize(data)\n",
    "rdd_text = spark.sparkContext.parallelize(text)\n",
    "\n",
    "data_partitions = rdd_data.getNumPartitions()\n",
    "text_partitions = rdd_text.getNumPartitions()\n",
    "\n",
    "print(f\"Number of partitions for numerical data is: {data_partitions}\")\n",
    "print(f\"Number of partitions for text data is: {text_partitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc749079-166f-4d33-9d11-72ba4375ba93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Apply a Transformation: Use an RDD transformation (such as map) on the RDD to create a new RDD. For example, if your RDD contains numbers, apply a function to square each number. (Recall that transformations create a new dataset from an existing one and are lazy, meaning they don’t execute immediately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c636571a-db8e-4d4a-826a-fb90ef051c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared Data RDD: [100, 400, 900, 1600, 2500, 3600, 4900, 6400, 8100, 10000]\nFiltered Data RDD ( > 50): [60, 70, 80, 90, 100]\nCount of elements in Data RDD: 10\nFirst 3 elements: [10, 20, 30]\n\nOriginal Text RDD: ['python', 'scala', 'sql', 'databricks', 'spark', 'aws', 'azure', 'machine learning']\nUppercase Text RDD: ['PYTHON', 'SCALA', 'SQL', 'DATABRICKS', 'SPARK', 'AWS', 'AZURE', 'MACHINE LEARNING']\nFiltered Text RDD (Words containing 'a'): ['scala', 'databricks', 'spark', 'aws', 'azure', 'machine learning']\nCount of words in Text RDD: 8\nFirst 2 words: ['python', 'scala']\n"
     ]
    }
   ],
   "source": [
    "# Transformations on Numerical RDD\n",
    "squared_rdd = rdd_data.map(lambda x: x**2)\n",
    "filtered_rdd = rdd_data.filter(lambda x: x >50)\n",
    "\n",
    "# Actions on Numerical RDD\n",
    "print(\"Squared Data RDD:\", squared_rdd.collect())  \n",
    "print(\"Filtered Data RDD ( > 50):\", filtered_rdd.collect())\n",
    "print(\"Count of elements in Data RDD:\", rdd_data.count()) \n",
    "print(\"First 3 elements:\", rdd_data.take(3))  \n",
    "\n",
    "# Transformations on Text RDD\n",
    "upper_rdd = rdd_text.map(lambda x : x.upper())\n",
    "filtered_text_rdd = rdd_text.filter(lambda x : 'a' in x.lower())\n",
    "\n",
    "# Actions on Text RDD\n",
    "print(\"\\nOriginal Text RDD:\", rdd_text.collect())\n",
    "print(\"Uppercase Text RDD:\", upper_rdd.collect())\n",
    "print(\"Filtered Text RDD (Words containing 'a'):\", filtered_text_rdd.collect())\n",
    "print(\"Count of words in Text RDD:\", rdd_text.count())\n",
    "print(\"First 2 words:\", rdd_text.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3103be57-299a-46e7-82be-633048832851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DataFrame Creation: Load a structured dataset into a DataFrame. You can use a provided CSV or JSON file (e.g., employees.csv with columns like id, name, department, salary), or create a small Pandas DataFrame and convert it to Spark DataFrame. Use spark.read (for CSV/JSON) or spark.createDataFrame (for existing data) to create the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ceec24-bd82-49db-8c28-f086aeb47d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------+\n| id|        name| department|salary|\n+---+------------+-----------+------+\n|  1|    John Doe|Engineering| 80000|\n|  2|  Jane Smith|         HR| 75000|\n|  3|Robert Brown|    Finance| 90000|\n|  4| Emily Davis|  Marketing| 70000|\n|  2|Robert Brown|         HR| 75000|\n|  3| Emily Davis|    Finance| 90000|\n|  3|Robert Brown|         HR| 70000|\n+---+------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"DataFrame Creation\").getOrCreate()\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "data = {\n",
    "    \"id\": [1, 2, 3, 4, 2, 3,3],\n",
    "    \"name\": [\"John Doe\", \"Jane Smith\", \"Robert Brown\", \"Emily Davis\", \"Robert Brown\",\"Emily Davis\",\"Robert Brown\"],\n",
    "    \"department\": [\"Engineering\", \"HR\", \"Finance\", \"Marketing\",\"HR\", \"Finance\",\"HR\"],\n",
    "    \"salary\": [80000, 75000, 90000, 70000, 75000, 90000, 70000]\n",
    "}\n",
    "\n",
    "pdf = pd.DataFrame(data)  # Create Pandas DataFrame\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "df_spark = spark.createDataFrame(pdf)\n",
    "\n",
    "# Show Spark DataFrame\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd584d3c-9780-4d1f-977f-50efd890a8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Inspect the DataFrame: Use DataFrame actions like printSchema() to display the schema and show(5) to display the first few rows. Verify that the data is loaded as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e3f82e-33d5-445a-8246-732ac16cbd79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+---+------------+-----------+------+\n| id|        name| department|salary|\n+---+------------+-----------+------+\n|  1|    John Doe|Engineering| 80000|\n|  2|  Jane Smith|         HR| 75000|\n|  3|Robert Brown|    Finance| 90000|\n|  4| Emily Davis|  Marketing| 70000|\n|  2|Robert Brown|         HR| 75000|\n+---+------------+-----------+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4926d0c-b7f3-437a-9421-7acb6c9adfb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DataFrame Transformation: Use DataFrame transformations to filter and transform the data. For example:\n",
    "• Filter the DataFrame to only include rows that meet a condition (e.g., salary > 50000).\n",
    "• Select specific columns or derive a new column (for instance, add 10% bonus to salary as a new column).\n",
    "• Perform an aggregation using groupBy and agg (e.g., group by department and compute average salary).\n",
    "Note that, similar to RDD transformations, DataFrame transformations are also lazy – they define a new query plan but do not execute until an action is invoked\n",
    "4. DataFrame Action: Invoke an action to materialize the results of your transformations. For example, use show() to display the aggregated results, or count() to count the filtered records. This will trigger the execution of the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e16852c-bb2c-4fce-971d-2c365f65ba3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame (Salary > 50000):\n+---+------------+-----------+------+\n| id|        name| department|salary|\n+---+------------+-----------+------+\n|  1|    John Doe|Engineering| 80000|\n|  2|  Jane Smith|         HR| 75000|\n|  3|Robert Brown|    Finance| 90000|\n|  4| Emily Davis|  Marketing| 70000|\n|  2|Robert Brown|         HR| 75000|\n|  3| Emily Davis|    Finance| 90000|\n|  3|Robert Brown|         HR| 70000|\n+---+------------+-----------+------+\n\nDataFrame with Bonus Column:\n+---+------------+-----------+------+-------+\n| id|        name| department|salary|  bonus|\n+---+------------+-----------+------+-------+\n|  1|    John Doe|Engineering| 80000|16000.0|\n|  2|  Jane Smith|         HR| 75000|15000.0|\n|  3|Robert Brown|    Finance| 90000|18000.0|\n|  4| Emily Davis|  Marketing| 70000|14000.0|\n|  2|Robert Brown|         HR| 75000|15000.0|\n|  3| Emily Davis|    Finance| 90000|18000.0|\n|  3|Robert Brown|         HR| 70000|14000.0|\n+---+------------+-----------+------+-------+\n\nAverage Salary per Department:\n+-----------+-----------------+\n| department|       avg_Salary|\n+-----------+-----------------+\n|Engineering|          80000.0|\n|         HR|73333.33333333333|\n|    Finance|          90000.0|\n|  Marketing|          70000.0|\n+-----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr, avg, sum\n",
    "\n",
    "filtered_df = df_spark.filter(col('salary') > 50)\n",
    "\n",
    "df_with_bonus = df_spark.withColumn('bonus', expr(\"salary * 0.2\"))\n",
    "\n",
    "avg_salary_df = df_spark.groupBy('department').agg(avg('salary').alias('avg_Salary'))\n",
    "\n",
    "print(\"Filtered DataFrame (Salary > 50000):\")\n",
    "filtered_df.show()\n",
    "\n",
    "print(\"DataFrame with Bonus Column:\")\n",
    "df_with_bonus.show()\n",
    "\n",
    "print(\"Average Salary per Department:\")\n",
    "avg_salary_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "475b3808-bdc5-4bf2-9627-46714dbf06d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Spark SQL Query: Register the DataFrame as a temporary view using createOrReplaceTempView(). Then, write and execute an SQL query using spark.sql() to achieve the same result as one of the transformations above (e.g., the same aggregation by department). Display the query result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0911acd-3404-40a6-90c0-5d499915fb81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Employees (Salary > 50000):\n+---+------------+-----------+------+\n| id|        name| department|salary|\n+---+------------+-----------+------+\n|  1|    John Doe|Engineering| 80000|\n|  2|  Jane Smith|         HR| 75000|\n|  3|Robert Brown|    Finance| 90000|\n|  4| Emily Davis|  Marketing| 70000|\n|  2|Robert Brown|         HR| 75000|\n|  3| Emily Davis|    Finance| 90000|\n|  3|Robert Brown|         HR| 70000|\n+---+------------+-----------+------+\n\nAverage Salary by Department:\n+-----------+-----------------+\n| department|       avg_salary|\n+-----------+-----------------+\n|Engineering|          80000.0|\n|         HR|73333.33333333333|\n|    Finance|          90000.0|\n|  Marketing|          70000.0|\n+-----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "filtered_result = spark.sql(\"SELECT * from employees where salary > 50000\")\n",
    "print(\"Filtered Employees (Salary > 50000):\")\n",
    "filtered_result.show()\n",
    "\n",
    "avg_salary_result = spark.sql(\"select department, avg(salary) as avg_salary from employees group by department\")\n",
    "\n",
    "print(\"Average Salary by Department:\")\n",
    "avg_salary_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6526868-4d1a-408a-85e3-2b9cf5e6f77e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "RDD from External Data: Using SparkContext, read an external text file into an RDD (for example, a log file or a text dataset). You can use sc.textFile(\"path/to/file.txt\") to create an RDD where each element is a line of the file. If you don’t have an external file, create a list of strings (e.g., sentences) and parallelize it as an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e75e758c-9d1e-4d02-9595-464fe8a83250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Contents:\n['Apache Spark is a fast and powerful engine for big data processing.', 'PySpark is the Python API for Apache Spark.', 'RDD stands for Resilient Distributed Dataset.', 'DataFrames are optimized compared to RDDs.', 'Spark SQL allows querying structured data using SQL.']\nOut[25]: 8"
     ]
    }
   ],
   "source": [
    "# Create a list of sentences\n",
    "text_data = [\n",
    "    \"Apache Spark is a fast and powerful engine for big data processing.\",\n",
    "    \"PySpark is the Python API for Apache Spark.\",\n",
    "    \"RDD stands for Resilient Distributed Dataset.\",\n",
    "    \"DataFrames are optimized compared to RDDs.\",\n",
    "    \"Spark SQL allows querying structured data using SQL.\"\n",
    "]\n",
    "\n",
    "# Parallelize the list into an RDD\n",
    "rdd = spark.sparkContext.parallelize(text_data)\n",
    "\n",
    "# Show the RDD contents\n",
    "print(\"RDD Contents:\")\n",
    "print(rdd.collect())\n",
    "\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e31ce0a-2b0a-4ff3-909a-770d9896bf73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. FlatMap and Map: If working with text data, use flatMap to split lines into words (so you get an RDD of individual words). Then use map to transform each word into a key-value pair of the form (word, 1). This prepares the data for counting word occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95cf3210-a528-4ca8-b9c8-414245afe931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words RDD:\n['Apache', 'Spark', 'is', 'a', 'fast', 'and', 'powerful', 'engine', 'for', 'big', 'data', 'processing.', 'PySpark', 'is', 'the', 'Python', 'API', 'for', 'Apache', 'Spark.', 'RDD', 'stands', 'for', 'Resilient', 'Distributed', 'Dataset.', 'DataFrames', 'are', 'optimized', 'compared', 'to', 'RDDs.', 'Spark', 'SQL', 'allows', 'querying', 'structured', 'data', 'using', 'SQL.']\nWord Pairs RDD:\n[('apache', 1), ('spark', 1), ('is', 1), ('a', 1), ('fast', 1), ('and', 1), ('powerful', 1), ('engine', 1), ('for', 1), ('big', 1), ('data', 1), ('processing.', 1), ('pyspark', 1), ('is', 1), ('the', 1), ('python', 1), ('api', 1), ('for', 1), ('apache', 1), ('spark.', 1), ('rdd', 1), ('stands', 1), ('for', 1), ('resilient', 1), ('distributed', 1), ('dataset.', 1), ('dataframes', 1), ('are', 1), ('optimized', 1), ('compared', 1), ('to', 1), ('rdds.', 1), ('spark', 1), ('sql', 1), ('allows', 1), ('querying', 1), ('structured', 1), ('data', 1), ('using', 1), ('sql.', 1)]\n"
     ]
    }
   ],
   "source": [
    "words_rdd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "word_pairs_rdd = words_rdd.map(lambda x: (x.lower(), 1))\n",
    "\n",
    "print(\"Words RDD:\")\n",
    "print(words_rdd.collect())\n",
    "\n",
    "print(\"Word Pairs RDD:\")\n",
    "print(word_pairs_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae35f76-81c9-4474-87d6-05484683be92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ReduceByKey: Apply the reduceByKey transformation on the key-value RDD to aggregate counts for each unique word (key). This will produce an RDD of (word, total_count) pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "563c49fc-739e-497e-8858-b68fe86ce4f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count Results:\n[('engine', 1), ('python', 1), ('spark.', 1), ('are', 1), ('optimized', 1), ('using', 1), ('sql.', 1), ('spark', 2), ('a', 1), ('powerful', 1), ('stands', 1), ('allows', 1), ('dataset.', 1), ('apache', 2), ('fast', 1), ('for', 3), ('data', 2), ('processing.', 1), ('resilient', 1), ('distributed', 1), ('rdds.', 1), ('api', 1), ('compared', 1), ('and', 1), ('pyspark', 1), ('the', 1), ('to', 1), ('structured', 1), ('is', 2), ('dataframes', 1), ('querying', 1), ('big', 1), ('rdd', 1), ('sql', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Apply reduceByKey to count word occurrences\n",
    "word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "#Show the word count results\n",
    "print(\"Word Count Results:\")\n",
    "print(word_counts_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e92e497f-ed7c-4db8-9247-1bd80e2c603d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Collect/Take Action: Use an action like collect() or take(10) to retrieve some of the word counts and print them. If the dataset is large, use take to print only a sample of the results (e.g., top 10 words) instead of the entire collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "337de4b3-f5f5-48cf-b5cd-d8d91147736b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Word Counts:\n[('engine', 1), ('python', 1), ('spark.', 1), ('are', 1), ('optimized', 1), ('using', 1), ('sql.', 1), ('spark', 2), ('a', 1), ('powerful', 1), ('stands', 1), ('allows', 1), ('dataset.', 1), ('apache', 2), ('fast', 1), ('for', 3), ('data', 2), ('processing.', 1), ('resilient', 1), ('distributed', 1), ('rdds.', 1), ('api', 1), ('compared', 1), ('and', 1), ('pyspark', 1), ('the', 1), ('to', 1), ('structured', 1), ('is', 2), ('dataframes', 1), ('querying', 1), ('big', 1), ('rdd', 1), ('sql', 1)]\n\nTop 10 Word Counts:\n[('engine', 1), ('python', 1), ('spark.', 1), ('are', 1), ('optimized', 1), ('using', 1), ('sql.', 1), ('spark', 2), ('a', 1), ('powerful', 1)]\n"
     ]
    }
   ],
   "source": [
    "all_word_counts = word_counts_rdd.collect()\n",
    "print(\"All Word Counts:\")\n",
    "print(all_word_counts)\n",
    "\n",
    "top_10_words = word_counts_rdd.take(10)\n",
    "print(\"\\nTop 10 Word Counts:\")\n",
    "print(top_10_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b091b32e-fc50-4973-abb2-196cb47262e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "1. Repeated Computation Scenario: Using the DataFrame or RDD from a previous exercise (or create a new one), design a scenario where you would perform multiple actions on the same dataset. For example, you might want to compute the count of records and also compute an aggregate (like sum or average on a column) on the same DataFrame/RDD. Write code to perform two or three different actions on the dataset without caching, and measure the execution time or observe the Spark job executions (if using Spark UI or logs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7288c0fa-015b-4c13-84ee-87261784b737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Expected Improvement with Caching\n",
    "- Without caching: \n",
    "Each action recomputes transformations, leading to slow performance.\n",
    "- With caching: The DataFrame is stored in memory, and subsequent actions reuse the cached data instead of recomputing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0056546-30cf-4e7f-80e9-af4210b5c183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "number of partitition?\n",
    "collect and take?\n",
    "reducebykey?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59b853d-2e4d-4918-9caf-0eb3d6dcd281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Record Count: 7 (Time taken: 0.3319 sec)\nTotal Salary Sum: 550000 (Time taken: 0.3800 sec)\nAverage Salary: 78571.43 (Time taken: 0.5928 sec)\n\nTotal Execution Time WITH Caching: 1.3047 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "df = df_spark\n",
    "\n",
    "# Now perform multiple actions\n",
    "start_time = time.time()\n",
    "record_count = df.count()  # First action\n",
    "count_time = time.time()\n",
    "\n",
    "sum_salary = df.select(sum(\"salary\")).collect()[0][0] # Second action\n",
    "sum_time = time.time()\n",
    "\n",
    "avg_salary = df.select(avg(\"salary\")).collect()[0][0]  # Third action\n",
    "avg_time = time.time()\n",
    "\n",
    "# Print results and execution times\n",
    "print(f\"Total Record Count: {record_count} (Time taken: {count_time - start_time:.4f} sec)\")\n",
    "print(f\"Total Salary Sum: {sum_salary} (Time taken: {sum_time - count_time:.4f} sec)\")\n",
    "print(f\"Average Salary: {avg_salary:.2f} (Time taken: {avg_time - sum_time:.4f} sec)\")\n",
    "\n",
    "# Measure total execution time\n",
    "total_time = avg_time - start_time\n",
    "print(f\"\\nTotal Execution Time WITHOUT Caching: {total_time:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45b4e6a4-5b7b-402e-a4ea-e1034be38bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Record Count: 7 (Time taken: 0.3345 sec)\nTotal Salary Sum: 550000 (Time taken: 0.4905 sec)\nAverage Salary: 78571.43 (Time taken: 0.3119 sec)\n\nTotal Execution Time WITH Caching: 1.1369 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cache the DataFrame to avoid redundant computations\n",
    "df_spark.cache()\n",
    "\n",
    "# Now perform multiple actions\n",
    "start_time = time.time()\n",
    "record_count = df_spark.count()  # First action\n",
    "count_time = time.time()\n",
    "\n",
    "sum_salary = df_spark.select(sum(\"salary\")).collect()[0][0] # Second action\n",
    "sum_time = time.time()\n",
    "\n",
    "avg_salary = df_spark.select(avg(\"salary\")).collect()[0][0]  # Third action\n",
    "avg_time = time.time()\n",
    "\n",
    "# Print results and execution times\n",
    "print(f\"Total Record Count: {record_count} (Time taken: {count_time - start_time:.4f} sec)\")\n",
    "print(f\"Total Salary Sum: {sum_salary} (Time taken: {sum_time - count_time:.4f} sec)\")\n",
    "print(f\"Average Salary: {avg_salary:.2f} (Time taken: {avg_time - sum_time:.4f} sec)\")\n",
    "\n",
    "# Measure total execution time\n",
    "total_time = avg_time - start_time\n",
    "print(f\"\\nTotal Execution Time WITH Caching: {total_time:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73f21d38-f3e9-4b01-a795-cb85cc42ced6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Expected Outcome\n",
    "Faster execution time with caching\n",
    "\n",
    "The first action (count()) will take longer since Spark loads and caches the DataFrame.\n",
    "Subsequent actions (sum(), avg()) will be faster as they reuse the cached data.\n",
    "Reduced Redundant Computations\n",
    "\n",
    "Unlike before, Spark won't recompute the entire DataFrame for each action.\n",
    "Verifying Cache Usage\n",
    "\n",
    "If using Spark UI, navigate to Storage Tab to confirm that the DataFrame is cached.\n",
    "Check logs for messages indicating that Spark is using cached data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "687d6123-5b8e-4b19-85a6-32b37768d7a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Compare Performance: If possible, compare the execution times or the Spark job DAG for the actions with and without caching. You should notice that with caching, the dataset is computed only once and reused for subsequent actions, whereas without caching Spark recomputes the dataset for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ddaeb24-ca8c-4cf2-b946-4fc4c77133bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Time Taken (sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Record Count</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.164051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Salary Sum</th>\n",
       "      <td>550000.000000</td>\n",
       "      <td>0.247052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Salary</th>\n",
       "      <td>78571.428571</td>\n",
       "      <td>0.215072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Execution Time WITH Caching</th>\n",
       "      <td>0.626175</td>\n",
       "      <td>0.626175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value</th>\n      <th>Time Taken (sec)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Total Record Count</th>\n      <td>7.000000</td>\n      <td>0.164051</td>\n    </tr>\n    <tr>\n      <th>Total Salary Sum</th>\n      <td>550000.000000</td>\n      <td>0.247052</td>\n    </tr>\n    <tr>\n      <th>Average Salary</th>\n      <td>78571.428571</td>\n      <td>0.215072</td>\n    </tr>\n    <tr>\n      <th>Total Execution Time WITH Caching</th>\n      <td>0.626175</td>\n      <td>0.626175</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, sum\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Step 1: Cache the DataFrame to avoid redundant computations\n",
    "df_spark.cache()  # or use \n",
    "\n",
    "# df_spark.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Step 2: Perform multiple actions WITH caching and measure execution time\n",
    "start_time = time.time()\n",
    "record_count_cached = df_spark.count()  # First action: Count records\n",
    "count_time_cached = time.time()\n",
    "\n",
    "sum_salary_cached = df_spark.select(sum(\"salary\")).collect()[0][0]  # Second action: Compute sum\n",
    "sum_time_cached = time.time()\n",
    "\n",
    "avg_salary_cached = df_spark.select(avg(\"salary\")).collect()[0][0]  # Third action: Compute average\n",
    "avg_time_cached = time.time()\n",
    "\n",
    "# Step 3: Print results and execution times\n",
    "execution_times_cached = {\n",
    "    \"Total Record Count\": (record_count_cached, count_time_cached - start_time),\n",
    "    \"Total Salary Sum\": (sum_salary_cached, sum_time_cached - count_time_cached),\n",
    "    \"Average Salary\": (avg_salary_cached, avg_time_cached - sum_time_cached),\n",
    "    \"Total Execution Time WITH Caching\": avg_time_cached - start_time\n",
    "}\n",
    "\n",
    "df_execution_times_cached = pd.DataFrame(execution_times_cached, index=[\"Value\", \"Time Taken (sec)\"]).T\n",
    "df_execution_times_cached\n",
    "# # Display execution times in a tabular format\n",
    "# tools.display_dataframe_to_user(name=\"Execution Times With Caching\", dataframe=df_execution_times_cached)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a80cdaf7-1d01-48bd-ae9c-a4f6acb49f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. When is Caching Beneficial?\n",
    "Caching in Spark improves performance when: \n",
    "- ✅ Multiple actions are performed on the same dataset → Without caching, Spark recomputes transformations every time an action is triggered.\n",
    "- ✅ The dataset is expensive to compute → If transformations involve costly operations (e.g., joins, aggregations), caching saves recomputation time.\n",
    "- ✅ The dataset fits in memory → If data can be stored in RAM (MEMORY_ONLY mode), subsequent operations are much faster.\n",
    "- ✅ Iterative processing is required → Common in machine learning and graph algorithms, where the same data is used repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "586d07a4-8e4f-42ef-8d1b-8e930ef03d0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. When Should You NOT Cache a Dataset?\n",
    "- ❌ When the dataset is used only once → If an RDD or DataFrame is used in a single action, caching adds unnecessary overhead.\n",
    "- ❌ When memory is limited → Caching large datasets in memory can cause OutOfMemory (OOM) errors, leading to job failures.\n",
    "- ❌ For small datasets → If the dataset is small, Spark can recompute it quickly, making caching unnecessary.\n",
    "- ❌ If transformations are cheap → Simple transformations (e.g., select() on a small dataset) don’t benefit significantly from caching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66954f20-610f-4796-a920-274c7f395ff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Importance of Unpersisting Cached Data (unpersist())\n",
    "- Cached data remains in memory until manually removed.\n",
    "- unpersist() frees up memory, preventing OOM errors when the dataset is no longer needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b457b90-e7a0-410e-874d-02fa20c49395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Final Takeaways\n",
    "- ✔ Caching reduces redundant computation, improving performance for repeated actions.\n",
    "- ✔ Choosing the right persistence level (MEMORY_ONLY, MEMORY_AND_DISK) ensures efficient memory management.\n",
    "- ✔ Unpersisting cached data (unpersist()) is essential to avoid memory issues in long-running Spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e3b89e8-9a66-4337-8a84-ffa9898da71a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 5: Spark Application Architecture (Conceptual Review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99409cb7-033c-4e18-a173-d048b3154ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Spark Architecture Components\n",
    "> A. Driver Program\n",
    "✅ What is it?\n",
    "\n",
    "The entry point of a Spark application.\n",
    "Runs on the Spark master node and creates the SparkSession.\n",
    "Converts user-defined operations (e.g., transformations, actions) into jobs.\n",
    "✅ Responsibilities:\n",
    "\n",
    "- Maintains SparkContext (interface between application and cluster).\n",
    "- Splits a job into stages based on transformations.\n",
    "- Schedules tasks and coordinates execution with executors.\n",
    "B. Executors\n",
    "> ✅ What are Executors?\n",
    "\n",
    "- Worker nodes that run tasks in parallel.\n",
    "- Each executor is assigned a fraction of data and processes it independently.\n",
    "**✅ Responsibilities:**\n",
    "\n",
    "- Executes tasks on assigned data partitions.\n",
    "- Stores intermediate results in memory (caching) if needed.\n",
    "- Reports task status and results back to the driver.\n",
    "### 2. Spark Execution Model\n",
    "**A. Jobs**\n",
    "\n",
    "✅ Definition:\n",
    "\n",
    "- A job is triggered whenever an action is called (e.g., .collect(), .count()).\n",
    "- The Spark driver splits the job into multiple stages.\n",
    "✅ Example:\n",
    "\n",
    "\n",
    "`df.count()  # Triggers a job`\n",
    "- This job counts rows in the DataFrame.\n",
    "- Spark converts this into tasks and sends them to executors.\n",
    "\n",
    "**B. Stages**\n",
    "\n",
    "✅ Definition:\n",
    "\n",
    "- A stage is a logical unit of computation determined by transformations.\n",
    "- Wide transformations (e.g., groupByKey(), reduceByKey()) trigger a shuffle, creating multiple stages.\n",
    "✅ Example of a Multi-Stage Execution:\n",
    "\n",
    "`rdd = sc.textFile(\"data.txt\")  # Stage 1\n",
    "\n",
    "word_counts = rdd.flatMap(lambda line: line.split(\" \"))  # Stage 1 (narrow transformation)\n",
    "\n",
    "word_pairs = word_counts.map(lambda word: (word, 1))  # Stage 1 (narrow transformation)\n",
    "\n",
    "word_freq = word_pairs.reduceByKey(lambda a, b: a + b)  # Stage 2 (wide transformation)\n",
    "\n",
    "word_freq.collect()  # Stage 2 (Action triggers execution)`\n",
    "\n",
    "- Stage 1: Reads data, splits into words, and maps them into key-value pairs.\n",
    "- Stage 2: Triggers a shuffle due to reduceByKey(), creating a new stage.\n",
    "**C. Tasks**\n",
    "✅ Definition:\n",
    "\n",
    "- A task is the smallest unit of execution.\n",
    "- Each stage is split into multiple tasks, each assigned to a partition.\n",
    "✅ Execution Flow:\n",
    "\n",
    "- The driver creates a job and breaks it into stages.\n",
    "- Each stage consists of multiple tasks.\n",
    "- Tasks are distributed to executors, where they run in parallel.\n",
    "- The driver collects results and returns them.\n",
    "### 3. How Spark’s Execution Model Works\n",
    "✅ Step-by-Step Breakdown\n",
    "\n",
    "- User Submits a Spark Job\n",
    "- The driver program starts execution.\n",
    "- SparkContext Creates an RDD or DataFrame\n",
    "- Lazy transformations (e.g., map(), filter()) do not trigger execution.\n",
    "- Spark Creates a DAG (Directed Acyclic Graph)\n",
    "- Splits execution into stages.\n",
    "- Stages Are Divided into Tasks\n",
    "- Spark assigns tasks to executors based on data locality.\n",
    "- Executors Process Tasks and Return Results\n",
    "- Executors store results in memory or disk if necessary.\n",
    "- Final Result Is Sent to the Driver\n",
    "- The action (e.g., collect(), count()) returns the computed result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d5536b7-b7ac-43ac-a1fc-b0aa7acfd3d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. DAG (Directed Acyclic Graph) in Spark\n",
    "\n",
    "✅ What is a DAG?\n",
    "\n",
    "- A DAG represents the logical execution plan of Spark operations.\n",
    "- Nodes represent RDD/DataFrame transformations.\n",
    "- Edges represent dependencies between transformations.\n",
    "\n",
    "✅ Key Benefits of DAG Execution:\n",
    "\n",
    "- Optimized execution → Spark reorders operations for efficiency.\n",
    "- Fault Tolerance → If a task fails, Spark can recompute only the failed partitions.\n",
    "- Parallelism → Tasks run in parallel across executors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e03ece3d-a8f9-4b7c-9b97-451682a88c73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day1_practise",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
